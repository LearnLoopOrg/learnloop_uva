overview = """
Types of machine learning

Supervised learning: classification & regression and unsupervised learning: clustering.

**Supervised Learning**

**Definition**: A type of machine learning where the model is trained on labeled data. The training dataset includes input-output pairs, and the model learns to make predictions or decisions based on this data. Example: predict housing prices based on the size of the house using historical data of house prices labeld with their size.

**Unsupervised Learning**

**Definition**: A type of machine learning where the model is trained on unlabeled data, and the system tries to learn the patterns and the structure from the data without any labels.

**Classification**

A supervised learning task that involves predicting the class or category of an object or sample based on its features. Classification algorithms are used when the output is a category such as ‘spam’ or ‘not spam’, ‘fraudulent’ or ‘not fraudulent’. The algorithm learns from the training data and applies the knowledge to classify new, unseen data. **Example: Email Spam Detection**: classifying emails as spam or not spam. **Algorithms**: **k-Nearest Neighbours (k-NN)**: Used for classifying objects based on closest training examples in the feature space. **Naive Bayes**: Used for classification with an assumption of independence among predictors.

**Naive Bayes**:

**Definition**: A classification technique based on Bayes’ Theorem with an assumption of independence between predictors. **More Information**: It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. This assumption simplifies the computation, and that’s why it is considered as "naive". **Example**: Spam email detection where the features are the presence of certain words, and the classes are spam and not spam.

**k-Nearest Neighbours (k-NN)**:

**Definition**: It classifies a data point based on how its neighbors are classified. The number of neighbors, *k*, is a user-defined constant. A distance metric, such as Euclidean distance, is used to find the  nearest neighbors. **Example**: Classifying emails as spam or not spam based on the similarity of their content to known spam and non-spam emails.

**Regression**

A supervised learning task that involves predicting a continuous quantity. Unlike classification, in regression, the output is a real or continuous value, such as ‘salary’, or ‘house price’. Regression algorithms predict the output values based on input features from the data fed in the system. **Examples**: **House Price Prediction**: Predicting the price of a house based on features like size, location, and number of bedrooms. Algorithms: Univariate Linear Regression, Multivariate Linear Regression, Univariate Polynomial Regression.

**Univariate Linear Regression**

**Definition**: A type of linear regression where there is a single independent variable to predict the dependent variable. **More Information**: It finds the linear relationship between the independent and dependent variables. **Example**: Predicting house prices based on size.

**Multivariate Linear Regression**

**Definition**: A type of linear regression with two or more independent variables. **More Information**: It finds the best fit hyperplane that predicts the dependent variable. **Example**: Predicting house prices based on size, number of bedrooms, and neighborhood.

**Univariate Polynomial Regression**

**Definition**: A type of regression where the relationship between the independent and dependent variable is modeled as an *n*th degree polynomial. **More Information**: Useful for describing curvilinear relationships. **Example**: Predicting the growth of bacteria as a function of temperature, where the relationship is not linear, but can be modeled as a polynomial.

**Clustering**

**Definition**: An unsupervised learning task that involves grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar to each other than to those in other groups. The goal is to segregate groups with similar traits and assign them into clusters. Clustering is used in various applications such as market research, pattern recognition, data analysis, and image processing. **Example: Customer Segmentation**: Grouping customers based on purchasing behavior. **Algorithm: k-Means.**

**k-Means**:

**Definition**: A partitioning method that divides a dataset into *k* clusters. **More Information**: Each observation belongs to the cluster with the nearest mean. The algorithm iteratively assigns points to clusters and recalculates the cluster centroids. **Example**: Customer segmentation based on purchasing behavior.
"""
k_nearest_neighbour = """
**k-nn (k-Nearest Neighbors)**: 

k-nn is a type of instance-based learning, or lazy learning, where the function is only approximated locally and all computation is deferred until function evaluation. It is useful for classification and regression tasks, relying on the similarity between data points in feature space to make predictions. Applications include image recognition, recommendation systems, and many areas of healthcare, such as predicting the progression of diseases.

**Classification**: 

k-NN is used for classification tasks, where the goal is to assign a label to a new, unclassified data point based on the labels of known data points. elaborate

**Effect of k**: 

The choice of k can significantly affect the classification result. Smaller values of k can be sensitive to noise, while larger values may smooth over details in the data. elaborate

**Single Nearest Neighbour**: 

The special case of k-NN when *k*=1, where the classification is based solely on the nearest known data point. elaborate

**Comparison to k-NN**: 

The NN classifier can be considered a subset of k-NN, with *k* fixed to 1. elaborate

**Distance Metric**: 

Euclidean distance is used to measure the straight-line distance between two points in space, which in turn determines their similarity.

**Features as Dimensions**: 

Each feature of a data point corresponds to a dimension in the space where the distance is calculated.

**Importance of Multiple Features**: Considering multiple features (dimensions) is crucial for accurately measuring similarity between data points, especially in complex datasets.

### **Voronoi Diagram**

A partitioning of a plane into regions based on the distance to a specific set of points. Each region consists of all the points closer to one specific data point than to any other. Used to visualize the regions of influence of each point in the training data, showing where the decision boundary lies.

### **Decision Boundary**

The boundary in the feature space where the predicted class changes from one class to another. It separates the regions associated with different classes.Points lying exactly on the decision boundary are equidistant from the nearest points of different classes, leading to classification ambiguity.

**Majority Voting**: 

The concept of determining the class of a test point in k-NN through majority voting among its *k* nearest neighbours.

**Handling Ties**: 

In case of a tie (equal number of nearest neighbours from each class), a decision rule needs to be defined to assign a class to the test point. 

Choosing k: **Precision vs. Robustness**

Increasing the value of kmay reduce the model's precision but make it more robust to noise and help prevent overfitting. The goal is to find a balance between precision and robustness, avoiding both overfitting and underfitting.

### **When to Use Small or Large k**

small k: Use for higher precision, especially when the data is clean and well-behaved. large k: Use to increase robustness against noise and to prevent overfitting, particularly in datasets with potential outliers or irregularities.
"""
naive_bayes = """**Bayes' Theorem**

De bayes theorem wordt vaak als counter intuitief ervaren en dat komt doordat mensen bij het berekenen van kansen vaak geen rekening houden met het feit dat bepaalde verschijnselen vele malen vaker voorkomen dan anderen. Een voorbeeld is de beschrijving van een man genaamd steve die een meek and tidy soul is en de vraag daarbij is of steve waarschijnlijk een librarian of een farmer is. De meeste mensen zullen zeggen librarian omdat ze die description daarbij vinden passen. Echter zegt Daniel khaneman dat dat irrationeel is gegeven dat er veel meer farmers zijn dan librarians en dus de kans dat deze beschrijving van een librarian is, veel kleiner is, dan van een farmer. Ookal acht je de kans dat de beschrijving zelf misschien bij 60% van de librarians past en maar tien procent van alle farmers zou passen. De aantallen van de farmers, maken de kans simpelweg veel groter dat de beschrijving van een farmer is, ookal is het percentage van de farmers die deze beschrijving fit erg klein.

### **Bayes' Rule**

Bayes' Rule is a mathematical formula used to update probabilities based on new evidence. It is expressed as: *P*(*H*∣*E*)=(*P*(*E*∣*H*)⋅*P*(*H*)) / *P*(*E*), where *P*(*H*∣*E*) is the posterior probability of hypothesis *H* given evidence *E*. *P*(*E*∣*H*) is the likelihood of observing evidence *E* if hypothesis *H* is true. *P*(*H*) is the prior probability of hypothesis *H*. *P*(*E*) is the probability of observing evidence *E*. The example of Steve being a librarian or a farmer illustrates how Bayes' Rule can lead to more accurate probability assessments by considering prior probabilities and the likelihood of evidence given a hypothesis.

### **Key Mantra Underlying Bayes' Theorem**

The key mantra underlying Bayes' Theorem is that new evidence should update prior beliefs, not replace them. Your updated belief (posterior probability) is a combination of your prior belief and the new evidence, weighted by how probable the evidence is under different hypotheses.

### **Rationality**

Rationality is not about knowing facts, its about knowing which facts are relevant. Rationality involves making decisions and forming beliefs based on logic, evidence, and consistency. It requires considering the relevance and weight of different pieces of information and updating beliefs appropriately when new evidence is presented.

### **Prior**

The prior probability, denoted as *P*(*H*), reflects your belief in the hypothesis before considering the new evidence. It is based on existing knowledge and information. For example, if there are significantly more farmers than librarians in the population, the prior probability of Steve being a farmer would be higher than that of him being a librarian. Another example is the spam filter. The prior in this case is the probability that an e-mail is spam or not, without knowing the contents of the email.

### **Likelihood**

The likelihood, denoted as *P*(*E*∣*H*), is the probability of observing the evidence given that the hypothesis is true. It reflects how well the evidence supports the hypothesis. In the Steve example, this would be the probability of observing the described characteristics if Steve is indeed a librarian (or a farmer).

### **Posterior**

The posterior probability, denoted as *P*(*H*∣*E*), is your updated belief in the hypothesis after taking the new evidence into account. It reflects how probable the hypothesis is, given both the prior information and the new evidence.

Naive bayes

Supervised classification algorithm that classifies an unknown datapoint used to predict a class or a label, similar to the k-NN algorithm. The main difference however is that knn relies on numerical data while naive bayes instead can be used to classifiy text. therefore it can make predictions of sequences of words.

Bayes rule for spam classification example

P(spam|text) = (P(text|spam)P(spam)) / P(text). This relates to the example of the librarians and farmers where …..

Input for naive bayes

The naive bayes algorithm needs a big dataset that contains an equal amount of emails that are classified as spam and an equal amount of emails that are classified as not spam.

Reason for the algorithm is naive 

The punctuation and the order of the words is ignored, eventhough this way a lot of valuable information is lost. In reality the words in the email and the purpose of the email is dependend on eachother. But ignoring this significantly simplifies the algorithm. That is why it is called the naive bayes algorithm. The naive bayes works by simply counting how many times a word occurs in emails with spam and in normal e-mails and then calculates the likelihoods for each word.
"""
univariate_linear = """**Univariate Linear Regression**: 

Univariate linear regression is a regression algorithm that models the relationship between a single independent variable and a dependent variable using a linear function. It is useful for understanding and predicting the outcome of a variable based on the value of another variable. Common applications include sales forecasting, risk assessment, and determining pricing strategies.

hypothesis:
A hypothesis in machine learning represents a proposed relationship between input features and the output. In univariate linear regression, the hypothesis is represented as *h*(*x*)=*ax*+*b*, where *a* is the slope, *b* is the y-intercept, and *x* is the input feature. The hypothesis aims to approximate the true underlying function that governs the data. For example, if we are predicting house prices based on size, our hypothesis might be *h*(*x*)=3000*x*+100,000, suggesting that for each additional square foot of size, the price increases by $3000.

cost function:
The cost function, also known as the loss function, calculates the difference between the predicted output and the actual output for all training examples. In univariate linear regression, a common cost function is the Mean Squared Error (MSE). The cost function produces a surface in the parameter space, and the goal is to find the lowest point on this surface, which corresponds to the optimal parameters for the hypothesis.

How the optimal *h*(*x*) (hypothesis) is determined:
The optimal hypothesis is determined by finding the values of *a* and *b* that minimize the cost function. This is equivalent to finding the lowest point on the cost function's surface in the parameter space. Various optimization algorithms, such as gradient descent, can be used to iteratively adjust the parameters to find this minimum.

gradient:
The gradient of a function at a particular point represents the rate of change of the function at that point, indicating the direction of the steepest increase. In the context of the cost function, the gradient points in the direction of the greatest increase in cost. The goal of gradient descent is to move in the opposite direction of the gradient to find the minimum cost. When the gradient is 0, it indicates a local minimum or maximum, and in the case of convex cost functions typical in linear regression, it corresponds to the global minimum.

mathematical approach:
The mathematical approach to finding the optimal parameters involves setting the gradient of the cost function to 0 and solving for the parameters. This intersection between the x-axis and the gradient function corresponds to the parameters for the hypothesis with the lowest cost. However, this analytical approach can be infeasible for complex models or large datasets, making iterative algorithms like gradient descent more practical.

convergence:
Convergence in the context of gradient descent refers to the algorithm iteratively adjusting the parameters to reach a point where further iterations result in negligible changes, signaling that a minimum of the cost function has been approached. The rate at which convergence occurs is influenced by the learning rate and the characteristics of the cost function. 

gradient descent

You can think of the process of gradient descent as analogous to finding the lowest point in a valley by always taking steps in the steepest downward direction, although the path might not always be direct and the algorithm could potentially get stuck in a local minimum.

learning rate:
The learning rate determines the size of the steps taken towards the minimum during each iteration of gradient descent. A large learning rate can lead to quicker convergence but risks overshooting the minimum, potentially causing the algorithm to diverge. A small learning rate ensures more careful progress but can result in a long convergence time. Finding the right balance is key to the efficient and successful application of gradient descent.

Update the theta values (parameters) simultaneously:

The theta (parameter) values should be updated simultaneously to ensure that all parameters are updated based on the same cost function value. If the parameters were updated sequentially, each parameter update would affect the subsequent updates within the same iteration, leading to skewed updates and potentially hindering convergence to the global minimum of the cost function.

how the univariate linear algorithm works

The univariate linear regression algorithm iteratively minimizes the cost function using gradient descent. In each iteration, gradients for parameters *a* and *b* are calculated to determine the direction of steepest descent. The parameters are then simultaneously updated, and the cost function is evaluated at the new parameter values. The size of the update step is monitored to ensure the algorithm is converging, with smaller steps indicating progress. The process repeats until the step size falls below a predetermined threshold, signaling convergence. At this point, the parameters *a* and *b* are considered optimized, providing the best linear fit to the data."""
multivariate_linear = """Multivariate Linear Regression: Multivariate linear regression is an extension of univariate linear regression that models the relationship between two or more features and a response by fitting a linear equation to observed data. It is useful for predicting a dependent variable based on multiple independent variables, providing a more nuanced and accurate prediction when multiple factors influence the outcome. Applications include predicting house prices based on various features, employee salary prediction, and many others in economics, biology, and engineering.
Difference between univariate and multivariate linear regression
Univariate linear regression involves predicting a single dependent variable based on one independent variable, represented by the equation y=mx+b. Multivariate linear regression, on the other hand, predicts a single dependent variable based on multiple independent variables, represented by the equation y=b0+b1x1+b2x2+…+bnxn. The primary difference is the number of independent variables involved in the prediction.
Why x0=1
In linear regression, x0 is set to 1 in order to include the intercept term (b0) in the model. By setting x0=1, the intercept term is effectively incorporated into the vector of parameters, allowing for a more compact and generalized matrix representation of the linear equation.
Steps in the multivariate linear regression algorithm
1) Initialize the parameters (theta values) randomly or with zeros. 2) Calculate the predicted values using the current parameters. 3) Compute the cost function to evaluate the error between predicted and actual values. 4) Calculate the gradient of the cost function with respect to each parameter. 5) Update the parameters by subtracting the product of the learning rate and the gradient. 6) Repeat steps 2-5 until convergence.
Partial derivative of the cost function
The partial derivative of the cost function with respect to a specific parameter represents how much the cost function changes as that parameter is adjusted. It is used in gradient descent to update the parameters in the direction that minimizes the cost function.
Feature scaling
Feature scaling is the process of normalizing or standardizing the range of independent variables or features in the data. It is useful because it brings all features to a similar scale, preventing features with larger scales from dominating the learning algorithm. Conceptually, it is done by subtracting the mean and dividing by the range or standard deviation of the feature.
Mean normalization
Mean normalization is a specific type of feature scaling where each value of a feature is transformed by subtracting the mean and dividing by the range of the feature. It is useful for speeding up learning and achieving faster convergence in gradient descent. Conceptually, it makes the features have approximately zero mean and values between -1 and 1.
Check convergence
Convergence in the context of optimization algorithms like gradient descent refers to the point at which further iterations no longer result in significant improvement or change in the parameters. To check if the algorithm is converging, one can monitor the cost function to ensure it is decreasing and approaching a minimum, and also check that the change in parameters between iterations is below a certain threshold.
Choice of amount of features
Adding more features to a model can help improve its predictive power, but only up to a certain point. The right amount of features balances model complexity and predictive accuracy, avoiding overfitting. Adding too many features, especially irrelevant or correlated ones, can lead to overfitting, where the model performs well on training data but poorly on unseen data.
Normal equation
The normal equation is a closed-form solution to find the optimal parameter values for linear regression, calculated as θ=(XTX)−1XTy. It directly computes the values that minimize the cost function, without the need for iterative optimization algorithms like gradient descent.
Differences between normal equation and gradient descent
Gradient descent is an iterative optimization algorithm that requires the choice of a learning rate and can handle a large number of features efficiently. The normal equation provides a direct solution without the need for iteration or choosing a learning rate, but it can be computationally expensive and slow for a very large number of features due to its O(n3) complexity.
Noise
Random variation in the data that is not part of the underlying pattern being modeled. This can arise from measurement error, unmeasured variables, or other sources of randomness. Noise can distort the true relationship between variables and make it more difficult to model the underlying pattern accurately.
Gaussian (Normal) Distribution
A continuous probability distribution characterized by its bell-shaped curve, symmetric about the mean. The mean determines the center of the graph, and the standard deviation determines the height and width of the graph. For example, human height can be modeled as a Gaussian distribution, with the average height being the most common (peak of the curve), and fewer people being extremely tall or short (tails of the curve).
Residuals
The differences between the predicted values and the actual values in a dataset. Residuals are used to assess the goodness of fit of a model, with smaller residuals indicating a better fit.
Standard Deviation (Sigma, σ)
A measure of the amount of variation or dispersion in a set of values. In the context of the Gaussian noise model, σ represents the spread of the noise, with a larger σ indicating more spread out values.
Probability Density Function (PDF)
A function that describes the likelihood of a random variable taking on a particular value. For continuous distributions, it describes the likelihood of a value falling within a certain range. The area under the PDF curve represents the probability, and the total area under the curve is equal to one.
"""
polynomial_regression = """**Polynomial Linear Regression**: 

Polynomial linear regression involves fitting a non-linear relationship between the independent and dependent variables using a polynomial function. Despite its name, it is a form of linear regression since the coefficients of the polynomial are linear parameters to be estimated from the data. It is useful for modeling relationships that are not linear, capturing more complex patterns in data. Applications include fields like economics for trend analysis, in environmental science for modeling biological processes, and in engineering for system behavior prediction.

**Maximum Likelihood Estimator (MLE)**: 

A method of estimating the parameters of a statistical model. MLE finds the parameter values that maximize the likelihood function, indicating the most probable parameters given the observed data.

**Link Between MLE and Cost Function**: 

The process of finding the Maximum Likelihood Estimator in a Gaussian noise model translates to minimizing the squared difference between the predicted and actual values, which is the cost function in linear regression. Conceptually, this creates a bridge between probabilistic modeling (MLE) and optimization (minimizing the cost function), showing that these two approaches can lead to the same parameter estimates.

**Nonlinear Feature Expansion**: 

A technique used to introduce non-linearity into a model by transforming original features or creating new features, enabling a linear model to capture complex relationships in the data. It is necessary when the relationship between the features and the target variable is not linear, as it allows for more flexible modeling of the data.

**Polynomial Feature Expansion**: 

A form of nonlinear feature expansion where original features are raised to various powers (squared, cubed, etc.) to create a polynomial relationship in the data. This is done by adding these powered terms as new features in the model, allowing for more complex, curved relationships to be modeled, even with a linear regression algorithm.

**Model Flexibility and Parameters**: 

As the number of parameters in a model increases, the model becomes more flexible and can adapt more closely to the nuances in the training data, potentially capturing more complex patterns.

**Cost on Training Data**: 

The cost or error on the training data typically decreases as the model becomes more flexible because the model can fit the training data more closely. However, if the model becomes too flexible, it may start to fit the noise in the training data, leading to overfitting.

**Overfitting**: 

A situation where a model fits the training data too closely, capturing the noise as if it were a real pattern, which results in poor performance on new, unseen data.

**Detecting Overfitting**: 

Overfitting can be detected by evaluating the model's performance on a validation set, a separate set of data not used during training. If the model performs well on the training data but poorly on the validation set, it may be overfitting.

**Validation Set**: 

A subset of the data set aside from the training data used to tune model parameters and select the best model, helping to prevent overfitting.

**Testing Set**: 

A separate subset of the data used to provide an unbiased evaluation of a final model fit, assessing how well the model will perform on unseen data.

**Training Set, Validation Set, and Testing Set**: 

In a comprehensive machine learning workflow, the training set is used to train the model, the validation set is used to tune parameters and select models, and the testing set is used to assess the performance of the selected model on new data.

**Challenges with Small Datasets**: 

With small datasets, splitting the data into training, validation, and testing sets can result in insufficient data for each purpose, potentially leading to unreliable models and performance estimates.

**K-Fold Cross-Validation**: 

A technique where the dataset is divided into k subsets, and the model is trained and validated k times, using a different subset as the validation set each time. This helps to maximize the use of available data for training and validation.

**Averaging Costs**: 

The costs from each fold in k-fold cross-validation are averaged to provide a more reliable and stable estimate of the model's performance.

**Use of All Data Points**: 

In k-fold cross-validation, each data point is used for validation exactly once, ensuring that all data contributes to the performance estimate and improving the reliability of the estimate.

**K-Fold Cross-Validation Repeated**: 

Repeating k-fold cross-validation multiple times with random shuffling of the data before each split can provide more reliable and stable estimates of training and validation costs, which is particularly beneficial with small datasets.

**Computing Average Training and Validation Costs**: 

Averaging training and validation costs across different polynomial degree models provides insights into how well each model generalizes from the training data to unseen data, helping to select the model with the right complexity.

**Model Complexity and Polynomial Degree**: 

The degree of a polynomial model correlates with its complexity; higher degree polynomials create more complex models that can fit the data more closely.

**Underfitting (High Bias)**: 

Occurs when a model is too simple to capture the underlying trend in the data, leading to high training and validation errors.

**Overfitting (High Variance)**: 

Occurs when a model is too complex, fitting the training data too closely and capturing noise as if it were a real pattern, leading to low training error but high validation error.

**Good Fit**: 

A model that has achieved a balance between complexity and simplicity, capturing the underlying trend in the data without fitting the noise, resulting in low training and validation errors.

**Bias**: 

The error introduced by approximating a real-world problem with a simplified model. High bias can lead to underfitting.

**Variance**: 
The variability in a model's predictions for a given data point. High variance can lead to overfitting.

**Bias-Variance Trade-off**: 
The balance between a model's ability to minimize bias and minimize variance. Considering the trade-off is crucial to develop models that generalize well to new data. A model with high bias (underfitting) will have a high error on both training and validation data, while a model with high variance (overfitting) will have low error on training data but high error on validation data.

**Improving Model Performance**: 

To enhance model performance, one should aim to find the right balance in the bias-variance trade-off. This can be achieved by adjusting model complexity, adding more data, or using techniques such as regularization."""


k_means = """**k-Means**: 

k-Means is a type of unsupervised learning algorithm used for clustering, which partitions data into k distinct, non-overlapping subsets (or clusters). The algorithm aims to minimize the variance within each cluster, while maximizing the variance between different clusters. It is widely used in market segmentation, document clustering, image segmentation, and anomaly detection.

**Clustering**: Clustering is the task of dividing the data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. In K-means, this is achieved by minimizing the within-cluster sum of squares.

**One-hot Encoding**: A representation of categorical variables as binary vectors. In the context of K-means, one-hot encoding can be used to represent cluster assignments, with a 1 indicating the cluster to which a data point is assigned and 0s indicating non-assignment.

**Euclidean Distance**: A measure of the straight line distance between two points in Euclidean space. In K-means, Euclidean distance is often used to calculate the distance between data points and cluster centroids to assign data points to the nearest cluster.

**Algorithm Initialization and Convergence**: K-means algorithm starts by initializing cluster centroids, then iteratively assigns data points to the nearest centroids, updates the centroids based on the current cluster assignments, and checks for convergence. If the centroids do not change significantly (within a specified tolerance), the algorithm has converged and the final clusters are formed.

**Initializing Cluster Means**: The initial cluster centroids are often selected randomly from the data points. This step is crucial as different initializations can lead to different final clusters. It is sometimes repeated multiple times with different initializations to find the best clustering.

**Assigning Clusters**: Each data point is assigned to the nearest cluster centroid based on a distance measure like Euclidean distance. This step ensures that each data point is grouped with the most similar points based on the current centroids.

**Updating Cluster Means**: After all data points are assigned to clusters, the centroids of each cluster are recalculated as the mean of all data points in the cluster. This step refines the cluster centroids to better represent the data points in each cluster.

**Convergence**: The algorithm repeats the steps of assigning clusters and updating cluster means until the centroids no longer change significantly, indicating that the algorithm has found stable clusters. A tolerance level is often set to determine when the change in centroids is small enough to declare convergence.

**Elbow Method**: The elbow method is a heuristic used in determining the number of clusters in a data set. The method consists of plotting the explained variation as a function of the number of clusters, and picking the “elbow” of the curve as the number of clusters to use. The elbow point represents a point of diminishing returns where increasing the number of clusters does not provide much better fit to the data."""
