[
    {
        "title": "overview",
        "glossary": """Types of machine learning 
        Supervised learning: classification & regression and unsupervised learning: clustering.
        **Supervised Learning**
        
        **Definition**: A type of machine learning where the model is trained on labeled data. The training dataset includes input-output pairs, and the model learns to make predictions or decisions based on this data. Example: predict housing prices based on the size of the house using historical data of house prices labeld with their size.
        
        **Unsupervised Learning**
        
        **Definition**: A type of machine learning where the model is trained on unlabeled data, and the system tries to learn the patterns and the structure from the data without any labels.
        
        **Classification**
        
        A supervised learning task that involves predicting the class or category of an object or sample based on its features. Classification algorithms are used when the output is a category such as ‘spam’ or ‘not spam’, ‘fraudulent’ or ‘not fraudulent’. The algorithm learns from the training data and applies the knowledge to classify new, unseen data. **Example: Email Spam Detection**: classifying emails as spam or not spam. **Algorithms**: **k-Nearest Neighbours (k-NN)**: Used for classifying objects based on closest training examples in the feature space. **Naive Bayes**: Used for classification with an assumption of independence among predictors.
        
        **Naive Bayes**:
        elg**        
      **Denition**: Aervised learning tas s b p (ca to each other than to those in other groups. The gggi
      """
        **Classification**: 
        k-NN is used for classification tasks, where the goal is to assign a label to a new, unclassified data point based on the labels of known data points. elaborate

        **Effect of k**: 

        The choice of k can significantly affect the classification result. Smaller values of k can be sensitive to noise, while larger values may smooth over details in the data. elaborate

        **Single Nearest Neighbour**: 

        The special case of k-NN when *k*=1, where the classification is based solely on the nearest known data point. elaborate

        **Comparison to k-NN**: 

        The NN classifier can be considered a subset of k-NN, with *k* fixed to 1. elaborate

        **Distance Metric**: 

        Euclidean distance is used to measure the straight-line distance between two points in space, which in turn determines their similarity.

        **Features as Dimensions**: 

        Each feature of a data point corresponds to a dimension in the space where the distance is calculated.

        **Importance of Multiple Features**: Considering multiple features (dimensions) is crucial for accurately measuring similarity between data points, especially in complex datasets.

        ### **Voronoi Diagram**

        A partitioning of a plane into regions based on the distance to a specific set of points. Each region consists of all the points closer to one specific data point than to any other. Used to visualize the regions of influence of each point in the training data, showing where the decision boundary lies.

        ### **Decision Boundary**

        The boundary in the feature space where the predicted class changes from one class to another. It separates the regions associated with different classes.Points lying exactly on the decision boundary are equidistant from the nearest points of different classes, leading to classification ambiguity.

        **Majority Voting**: 

        The concept of determining the class of a test point in k-NN through majority voting among its *k* nearest neighbours.

        **Handling Ties**: 

        In case of a tie (equal number of nearest neighbours from each class), a decision rule needs to be defined to assign a class to the test point. 

        Choosing k: **Precision vs. Robustness**

        Increasing the value of kmay reduce the model's precision but make it more robust to noise and help prevent overfitting. The goal is to find a balance between precision and robustness, avoiding both overfitting and underfitting.

        ### **When to Use Small or Large k**

        small k: Use for higher precision, especially when the data is clean and well-behaved. large k: Use to increase robustness against noise and to prevent overfitting, particularly in datasets with potential outliers or irregularities.
        """
    }
]